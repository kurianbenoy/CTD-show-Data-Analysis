{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode = pd.read_csv('input/Episodes.csv')\n",
    "desc = pd.read_csv('input/Description.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "78    2020-05-27\nName: release_date, dtype: object"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "episode[episode['episode_id']==\"E69\"][\"release_date\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If you have listened to that episode, Sanyanam had said he has turned into age of 23. So his DOB is: 27/05/1997"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_transcript(fn, save=False, save_path=''):\n",
    "    \"Takes transcript and converts it to `DataFrame`\"\n",
    "    pat = r'([A-Za-z]|\\s+)\\s([0-9]{0,2}:{0,1}[0-9]{1,2}:[0-9][0-9])'\n",
    "    f = open(fn, \"r\")\n",
    "    t = True\n",
    "    df = pd.DataFrame(columns = ['Time', 'Speaker', 'Text'])\n",
    "    i = 0\n",
    "    first = True\n",
    "    while t:\n",
    "        line = f.readline()\n",
    "        if line == '': t = False\n",
    "        i += 1\n",
    "        line = re.split(pat, line[:-1])\n",
    "        if len(line) == 4:\n",
    "            is_new = 1\n",
    "            speak = line[0]\n",
    "            time = line[2]\n",
    "        while is_new == 1:\n",
    "            if first:\n",
    "                line = f.readline()\n",
    "                for i in range(6):\n",
    "                    l_c = f.readline()\n",
    "                    if speak not in l_c and time not in l_c:\n",
    "                        line += l_c\n",
    "                i += 1\n",
    "                first = False\n",
    "            else:\n",
    "                line = f.readline()\n",
    "                i += 1\n",
    "            if len(line) > 2 and line != '\\n':\n",
    "                line = line[:-1]\n",
    "                df.loc[i] = [time, speak, line]\n",
    "                df.reset_index()\n",
    "            else:\n",
    "                is_new = 0\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df['Text'] = df['Text'].replace('\\n', '')\n",
    "    if save:\n",
    "        df.to_csv(save_path+fn.name[:-3] + 'csv', index=False, sep='|')\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_transcript(fn, save=False, save_path=''):\n",
    "    \"Takes transcript and converts it to `DataFrame`\"\n",
    "    pat = r'([A-Za-z]|\\s+)\\s([0-9]{0,2}:{0,1}[0-9]{1,2}:[0-9][0-9])'\n",
    "    f = open(fn, \"r\")\n",
    "    t = True\n",
    "    df = pd.DataFrame(columns = ['Time', 'Speaker', 'Text'])\n",
    "    i = 0\n",
    "    first = True\n",
    "    while t:\n",
    "        line = f.readline()\n",
    "        print(len(line))\n",
    "        if line == '': t = False\n",
    "        i += 1\n",
    "        line = re.split(pat, line[:-1])\n",
    "        if len(line) == 4:\n",
    "            is_new = 1\n",
    "            speak = line[0]\n",
    "            time = line[2]\n",
    "        while is_new == 1:\n",
    "            if first:\n",
    "                line = f.readline()\n",
    "                for i in range(6):\n",
    "                    l_c = f.readline()\n",
    "                    if speak not in l_c and time not in l_c:\n",
    "                        line += l_c\n",
    "                i += 1\n",
    "                first = False\n",
    "            else:\n",
    "                line = f.readline()\n",
    "                i += 1\n",
    "            if len(line) > 2 and line != '\\n':\n",
    "                line = line[:-1]\n",
    "                df.loc[i] = [time, speak, line]\n",
    "                df.reset_index()\n",
    "            else:\n",
    "                is_new = 0\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df['Text'] = df['Text'].replace('\\n', '')\n",
    "    if save:\n",
    "        df.to_csv(save_path+fn.name[:-3] + 'csv', index=False, sep='|')\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_filenames(path):\n",
    "    dir_name = []\n",
    "    for f_name in os.listdir(path):\n",
    "        dir_name.append(f_name)\n",
    "    return dir_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "li = get_filenames('input/Cleaned Subtitles/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "23\n24\n23\n24\n23\n24\n23\n24\n23\n24\n23\n24\n23\n24\n23\n24\n23\n24\n23\n24\n23\n24\n23\n24\n23\n24\n23\n24\n23\n24\n23\n24\n23\n24\n23\n24\n23\n24\n23\n24\n23\n24\n24\n23\n24\n23\n24\n23\n24\n23\n24\n23\n24\n24\n25\n24\n25\n24\n25\n24\n25\n24\n25\n24\n25\n24\n25\n24\n25\n24\n25\n24\n25\n24\n25\n24\n25\n24\n25\n24\n25\n24\n25\n24\n25\n24\n25\n24\n24\n25\n24\n25\n24\n25\n24\n25\n24\n25\n24\n25\n24\n25\n24\n25\n24\n25\n24\n25\n24\n25\n24\n25\n24\n25\n25\n24\n25\n24\n25\n24\n24\n25\n24\n25\n24\n25\n24\n25\n24\n25\n24\n25\n24\n25\n24\n25\n24\n25\n24\n25\n24\n24\n25\n24\n25\n24\n25\n24\n25\n24\n25\n24\n25\n24\n25\n24\n25\n24\n25\n24\n25\n24\n25\n24\n25\n24\n25\n24\n25\n24\n25\n24\n25\n24\n25\n24\n25\n24\n25\n24\n25\n24\n25\n24\n25\n24\n25\n24\n25\n24\n25\n24\n25\n24\n25\n24\n25\n24\n25\n24\n25\n24\n25\n24\n25\n24\n25\n24\n25\n24\n25\n24\n25\n24\n25\n24\n25\n24\n25\n24\n25\n24\n25\n24\n25\n24\n0\n"
    }
   ],
   "source": [
    "df = extract_transcript('input/Raw Subtitles/E1.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   Time          Speaker                                               Text\n0  0:13   Sanyam Bhutani  Hey, this is Sanyam Bhutani and you're listeni...\n1  1:49  Abhishek Thakur  Thank you very much for the invitation. It's a...\n2  1:53   Sanyam Bhutani  Today, you're the world's only Triple Grandmas...\n3  2:12  Abhishek Thakur  Yeah cool story. Data science was never my int...\n4  2:41   Sanyam Bhutani  And this was before the boom had happened. And...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Time</th>\n      <th>Speaker</th>\n      <th>Text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0:13</td>\n      <td>Sanyam Bhutani</td>\n      <td>Hey, this is Sanyam Bhutani and you're listeni...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1:49</td>\n      <td>Abhishek Thakur</td>\n      <td>Thank you very much for the invitation. It's a...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1:53</td>\n      <td>Sanyam Bhutani</td>\n      <td>Today, you're the world's only Triple Grandmas...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2:12</td>\n      <td>Abhishek Thakur</td>\n      <td>Yeah cool story. Data science was never my int...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2:41</td>\n      <td>Sanyam Bhutani</td>\n      <td>And this was before the boom had happened. And...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   Time          Speaker                                               Text\n0  0:13   Sanyam Bhutani  Hey, this is Sanyam Bhutani and you're listeni...\n1  1:49  Abhishek Thakur  Thank you very much for the invitation. It's a...\n2  1:53   Sanyam Bhutani  Today, you're the world's only Triple Grandmas...\n3  2:12  Abhishek Thakur  Yeah cool story. Data science was never my int...\n4  2:41   Sanyam Bhutani  And this was before the boom had happened. And...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Time</th>\n      <th>Speaker</th>\n      <th>Text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0:13</td>\n      <td>Sanyam Bhutani</td>\n      <td>Hey, this is Sanyam Bhutani and you're listeni...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1:49</td>\n      <td>Abhishek Thakur</td>\n      <td>Thank you very much for the invitation. It's a...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1:53</td>\n      <td>Sanyam Bhutani</td>\n      <td>Today, you're the world's only Triple Grandmas...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2:12</td>\n      <td>Abhishek Thakur</td>\n      <td>Yeah cool story. Data science was never my int...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2:41</td>\n      <td>Sanyam Bhutani</td>\n      <td>And this was before the boom had happened. And...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "episode_1 = pd.read_csv('input/Cleaned Subtitles/E1.csv')\n",
    "episode_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "   Time         Speaker                                               Text\n0  0:13  Sanyam Bhutani  Hey, this is Sanyam Bhutani and you're listeni...\n1  1:46  Sanyam Bhutani  Hi, everyone, I'm today talking to a very spec...\n2  1:57    Ryan Chesler  Yeah, yeah. Thanks for having me. I've seen yo...\n3  2:06  Sanyam Bhutani  So I'm really a fan of your write up, the pers...\n4  2:32    Ryan Chesler  Yeah, yeah. It really was just looking at YouT...",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Time</th>\n      <th>Speaker</th>\n      <th>Text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0:13</td>\n      <td>Sanyam Bhutani</td>\n      <td>Hey, this is Sanyam Bhutani and you're listeni...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1:46</td>\n      <td>Sanyam Bhutani</td>\n      <td>Hi, everyone, I'm today talking to a very spec...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1:57</td>\n      <td>Ryan Chesler</td>\n      <td>Yeah, yeah. Thanks for having me. I've seen yo...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2:06</td>\n      <td>Sanyam Bhutani</td>\n      <td>So I'm really a fan of your write up, the pers...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2:32</td>\n      <td>Ryan Chesler</td>\n      <td>Yeah, yeah. It really was just looking at YouT...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "episode_2 = pd.read_csv('input/Cleaned Subtitles/E2.csv')\n",
    "episode_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'Hey, this is Sanyam Bhutani and you\\'re listening to \"Chai Time Data Science\", a podcast for data science enthusiasts, where I interview practitioners, researchers, and Kagglers about their journey, experience, and talk all things about data science.\\n\\nSanyam Bhutani  0:46  \\nHello, and welcome to the first episode. This is also part 26 of interview with machine learning heroes. In this episode, I\\'m honored to be joined by Abhishek Thakur, chief data scientist at boost.ai, and also the world\\'s first, and at the time of recording, only triple Grand Master on Kaggle. Abhishek has been working as a data scientist for the past few years. He also has a background in computer science with a master\\'s degree from the University of Bonn. We talked about his journey into data science, his Kaggle experience and his current projects. Enjoy the show.\\n\\nSanyam Bhutani  1:38  \\nHi, everyone. Thanks for tuning in. I\\'m really honored to be talking to the world\\'s first triple Grand Master today. Thank you so much for taking the time to do this interview Abhishek.'"
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "episode_1['Text'].loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "episode_n = pd.read_csv('input/Cleaned Subtitles/E75.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "intro_n = episode_n['Text'].loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "1852"
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "len(intro_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "1056"
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "intro_1 = episode_1['Text'].loc[0]\n",
    "len(intro_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What are the future plans for Chai time data science?\n",
    "\n",
    "- what are the top enhancements or changes you're working on for CTDS? Anything to look forward to, in the near future?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ama_episode = pd.read_csv('input/Cleaned Subtitles/E69.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "'The next question is from Rohan Rao who\\'s a data scientist at h2o Kaggle Grand Master and also worldwide known sodoku champion, what are the top enhancements or changes you\\'re working on for CTDS? Anything to look forward to, in the near future? Uh funny story. The branding idea of CTDS actually came from Rohan, thanks to his suggestion. And yes, there are a lot of exciting things that I am I\\'ve been working on. So three things. I\\'m launching a new podcast. Yes, a new podcast called \"Chai Time Data Science News\" CTDS.news. You can find another write up that will live with this blog post. You can read all about it. But the idea is to give you a short news podcast, ideally in three to five minutes, always less than 10 minutes for you to be able to be on top of data science news, basically. Now if you know me, I am completely community driven. So if you have any thoughts around around that, please let me know the first episode is supposed to go live somewhere in the first week of June, so in a few days from now, but we\\'ll see how that goes. Secondly, you as you might know, I have been subtitling, every single one of the interview that goes out is all of the two interviews that go out and have been going on since January. And now I will be starting a blog version release of these also in the first week of June or probably before that. So you can read you can expect blog releases and also short, interesting parts of the conversation short clips. I was going to call it cutting chai shorts from \"Chai Time Data Science.\\' So really excited about that. I also have a new set a new camera a new mic my new look. But we\\'ll see how that goes. The \"Chai Time Data Science\" story, how did it all begin? So, I have been interviewing my heroes on machine learning in a blog format. And I started that in my third year of studies third year of university. I did almost 25 interviews but after a point did it start I started feeling it was getting repetitive. My questions were getting repetitive and I I didn\\'t want to just go out there and waste these amazing people that were giving me their time, their time. Also, to me it was feeling like I was asking these heroes of mine to write the blog posts for me It means that that\\'s how it would fit into my pipe and I\\'ll send them in questions they would write the answers back. Another thing that I really wanted to do was I, luckily, was able to interview many Kaggle Grand Masters many Kaggle Masters. And I wanted to apply their advice myself. So this might not happen in parallel, but I focus more on Kaggle, I really focus on Kaggle at least so six, seven months that I was active on the platform. And right around the last few months of my activity, I actually made it to the finals of the Google air residency programme. So I was also freelancing at that time freelancing because you\\'re not allowed to have a job job in college. And I had opted out of college placement. So in India, your college helps you get a job. I had opted out of that very boldly because I didn\\'t see any machine learning related roles. I was gonna my I thought might as well just get if I really want to do this if, if I\\'m not confident enough, why dare I say that I\\'d love to be a part of this field. So I did that. And if you have worked in consulting or freelancing, you know, when it rains, it pours. And when it doesn\\'t, it\\'s drier than a desert. So the Google a residency'"
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "ama_episode['Text'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And yes, there are a lot of exciting things that I am I\\'ve been working on. So three things. I\\'m launching a new podcast. Yes, a new podcast called \"Chai Time Data Science News\" CTDS.news. You can find another write up that will live with this blog post. You can read all about it. But the idea is to give you a short news podcast, ideally in three to five minutes, always less than 10 minutes for you to be able to be on top of data science news, basically. Now if you know me, I am completely community driven. So if you have any thoughts around around that, please let me know the first episode is supposed to go live somewhere in the first week of June, so in a few days from now, but we\\'ll see how that goes. Secondly, you as you might know, I have been subtitling, every single one of the interview that goes out is all of the two interviews that go out and have been going on since January. And now I will be starting a blog version release of these also in the first week of June or probably before that. So you can read you can expect blog releases and also short, interesting parts of the conversation short clips. I was going to call it cutting chai shorts from \"Chai Time Data Science.\\' So really excited about that. I also have a new set a new camera a new mic my new look. But we\\'ll see how that goes. The \"Chai Time Data Science\" story, how did it all begin? So, I have been interviewing my heroes on machine learning in a blog format. And I started that in my third year of studies third year of university. I did almost 25 interviews but after a point did it start I started feeling it was getting repetitive. My questions were getting repetitive and I I didn\\'t want to just go out there and waste these amazing people that were giving me their time, their time. Also, to me it was feeling like I was asking these heroes of mine to write the blog posts for me It means that that\\'s how it would fit into my pipe and I\\'ll send them in questions they would write the answers back. Another thing that I really wanted to do was I, luckily, was able to interview many Kaggle Grand Masters many Kaggle Masters. And I wanted to apply their advice myself. So this might not happen in parallel, but I focus more on Kaggle, I really focus on Kaggle at least so six, seven months that I was active on the platform. And right around the last few months of my activity, I actually made it to the finals of the Google air residency programme. So I was also freelancing at that time freelancing because you\\'re not allowed to have a job job in college. And I had opted out of college placement. So in India, your college helps you get a job. I had opted out of that very boldly because I didn\\'t see any machine learning related roles. I was gonna my I thought might as well just get if I really want to do this if, if I\\'m not confident enough, why dare I say that I\\'d love to be a part of this field. So I did that. And if you have worked in consulting or freelancing, you know, when it rains, it pours. And when it doesn\\'t, it\\'s drier than a desert. So the Google a residency'"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python_defaultSpec_1593889089394",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}